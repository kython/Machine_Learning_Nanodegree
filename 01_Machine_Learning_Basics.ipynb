{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习基础\n",
    "## 什么是机器学习\n",
    "- 决策树 Decision Tree: APP推荐\n",
    "- 朴素贝叶斯 Naive Bayes Rule: 垃圾邮件判别\n",
    "- 梯度下降 Gradient Decent\n",
    "- 线性回归 Linear Regression: 房价预测 连续值\n",
    "- 对数几率回归 Logistic Regression: 学生录取 离散值 分类问题\n",
    "- 支持向量机 Support Vector Machine\n",
    "- 核函数 Kernel Trick: 用于SVM；用一条曲线分类数据或者数据转化到高维空间用平面划分。\n",
    "- 神经网络 Neural Network: 学生录取\n",
    "- K均值聚类 K-means: 面包店选址\n",
    "- 层次聚类 Hierarchical Clustering: 一种解决K-means算法中k难以确定的聚类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置Anaconda\n",
    "- Anaconda[下载网址](https://www.continuum.io/downloads)\n",
    "- 本次安装的python 3.6 version Graphical Installer\n",
    "- 安装后不能直接在命令行使用`conda`指令，需在.bashrc加入`export PATH=\"/<path to anaconda>/bin:$PATH\"`，详情参考Anaconda[安装指南](https://docs.continuum.io/anaconda/install/mac-os#macos-graphical-install)\n",
    "- 常用conda指令：\n",
    "    - 当前环境包管理信息 `conda list`\n",
    "    - 创建环境 `conda create -n my_env python=2`\n",
    "    - 已有环境列表 `conda env list`\n",
    "    - 进入环境（Mac) `source activate my_env`\n",
    "    - 退出环境 `deactivate`\n",
    "    - 安装包 `conda install pkg/like/numpy`\n",
    "    - conda帮助 `conda -h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置Jupyter notebook\n",
    "- 在当前环境安装jupyter notebook `conda install jupyter notebook`\n",
    "- 启动 notebook 服务器，在终端或控制台中输入 jupyter notebook\n",
    "- 命令行模式：单元格为蓝色；编辑模式：单元格为绿色\n",
    "- 命令行模式下神器 comand platte 呼出快捷键 `cmd/ctrl+shift+p` 或者直接 `p`\n",
    "- 命令行模式下查看快捷键列表：`h`\n",
    "- Magic 关键字是可以在单元格中运行的特殊命令。例如，在 notebook 中可以使用 %matplotlib 将 matplotlib 设置为以交互方式工作\n",
    "- Magic 命令的前面带有一个或两个百分号（% 或 %%），分别对应行 Magic 命令和单元格 Magic 命令。行 Magic 命令仅应用于编写 Magic 命令时所在的行，而单元格 Magic 命令应用于整个单元格\n",
    "- 代码计时 %timeit %%timeit\n",
    "- 更多[Magic列表](http://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "- 支持数学公式，如：$a=\\frac{b}{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的中心：众数，平均数和中位数\n",
    "### 众数 Mode\n",
    "> 定义：一组数据中出现次数最多的数值，众数可以不存在也可以多于一个。\n",
    "\n",
    "- 数据集中不是所有的分值都会影响众数\n",
    "- 从统一分布中抽取出的一组样本，其众数不一定相同，且众数的取值受分组大小（bin)的影响。\n",
    "- 众数没有一个计算公式\n",
    "\n",
    "### 平均数 Mean\n",
    "**定义**：\n",
    "\n",
    "采样中记为：$$\\overline{x}=\\frac{\\sum_{i=1}^{n}{x_i}}{n}$$\n",
    "\n",
    "分布中记为：$$\\mu=\\frac{\\sum{x}}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "geo = [48670, 57320, 38150, 41290, 53160,500000]\n",
    "mean_geo = np.sum(geo)/len(geo)\n",
    "print(mean_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均数特性：\n",
    "\n",
    "- 分布中的所有分值都影响平均值\n",
    "- 平均值可用公式来描述\n",
    "- 同一个总体中的多个样本会有相似的平均值\n",
    "- 一个样本的平均值可以用来推论其所在的总体\n",
    "- 如果向数据集中添加一个极值，它的平均值会发生改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![平均数示意](img/mean.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "含异常（outlier）的均值，不能良好的评估数据集。一个有趣的例子是，北卡莱罗纳大学地理专业毕业生的平均薪资远高于该专业其他大学的毕业生，原因竟然是因为北卡莱罗纳大学地理专业毕业生中有一位名叫乔丹的明星，他的个人薪资极大地拉高了该专业的平均薪资。因此异常值的出现会影响用平均值评估数据集。\n",
    "\n",
    "这就是为什么需要引入中位数。**中位数受异常值影响较小。**\n",
    "\n",
    "### 中位数 Median\n",
    "> 中位数是位于“中间”的数据，意味着有一半数据值小于它，而另一半大于它。\n",
    "\n",
    "[婚庆行业的价格秘密](http://www.slate.com/articles/life/weddings/2013/06/average_wedding_cost_published_numbers_on_the_price_of_a_wedding_are_totally.html)举例说明了知道何时和如何使用中位数及均值，是多么重要。这个例子和婚礼费用有关，向你展示了为什么平均婚礼费用不适用于说明普通夫妇的婚礼开支。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     241\n",
       "2    1116\n",
       "3      69\n",
       "4    1214\n",
       "Name: friends, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fb_friends = pd.read_csv(\"data/How_many_Facebook_friends_do_Udacians_have.csv\")\n",
    "fb_friends['friends'].median()\n",
    "fb_friends.iloc[1:5,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的差异性：值域，IQR，方差和标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 量化数据的分布形态\n",
    "- 最大值和最小值之间的差（值域）\n",
    "- 当异常值出现时，值域不能良好的代表数据的差异性\n",
    "\n",
    "#### 砍掉尾巴\n",
    "虽然 Katie 建议去除上下各 25% 的数据点，但我们在去除数据点时还需要特别谨慎，特别是在数据量不大的情况下，去除一半的数据点会让我们丢失大量数据的信息。\n",
    "\n",
    "一般来说，在去除数据点前，我们建议首先将数据点通过图像表述出来（直方图、散点图、箱线图等），这样可以帮助你获得对数据整理分析的了解。然后，基于项目的背景，判断你更关心数据的哪一部分（大多数正常数据，还是小部分异常数据），因为在一些项目背景下，你可能更关心那些异常值，比如在 数据分析（进阶）课程的安然数据分析中，那些工资异常高的人更有可能腐败。最后，是基于现有的数据量作出决定，究竟是直接丢弃部分数据还是对部分作出调整，亦或是有保留地接受所有数据。特别记住一点，没有一种分析方法100%正确，但我们总可以尝试根据不同的需求找到一种最合理的方法。\n",
    "\n",
    "#### IQR = Q3-Q1 Interquartile range\n",
    "第三个四分之一减去第一个四分之一\n",
    "- 几乎 50% 的数据在 IQR 间\n",
    "- IQR 不受异常值的影响\n",
    "\n",
    "#### 异常值 Outlier\n",
    "$$ Outlier < Q_1 - 1.5IQR $$\n",
    "$$ Outlier > Q_3 + 1.5IQR $$\n",
    "\n",
    "#### 箱线图 box and whisker\n",
    "![箱线图](img/boxandwhisker.jpg)\n",
    "\n",
    "**问**：均值一定在IRQ中吗？\n",
    "\n",
    "**答**：No，均值受Outlier影响严重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 衡量差异性的方法\n",
    "- 找出每个值与数据集均值之间差的平均值\n",
    "- 正偏差和负偏差相互抵消\n",
    "- 绝对偏差\n",
    "\n",
    "**平均绝对偏差**：\n",
    "\n",
    "$$ \\frac{\\sum{|x_i-\\overline{x}|}}{n}$$\n",
    "\n",
    "**SS Sum of Squre 平方和**：\n",
    "\n",
    "$$ \\sum{(x_i-\\overline{x}})^2$$\n",
    "\n",
    "**方差 Variance**：\n",
    "\n",
    "$$ \\frac{\\sum{(x_i-\\overline{x}})^2}{n}$$\n",
    "\n",
    "![方差](img/variance.jpg)\n",
    "\n",
    "**标准偏差 Standard Deviation**：\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$\n",
    "\n",
    "![标准偏差](img/standard_deviation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝塞尔校正\n",
    "我们发现从采样样本中计算出的偏差一般比原始数据小。这是因为在正态分布中，采样数据倾向于取峰值附近的数据，因此采样数据的偏差比原始数据小得多，如下图所示：\n",
    "![Bessel Correction](img/bessel_correction.jpg)\n",
    "\n",
    "为了矫正以上误差，我们一般用贝塞尔矫正修正偏差公式。这就是为什么以前书上写的标准差公式的分母为n-1的原因。\n",
    "**样本标准方差**：\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n-1}}$$\n",
    "\n",
    "**分布标准方差**：\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$\n",
    "\n",
    "#### 澄清样本SD值的真正含义\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n-1}} \\approx \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 监督学习简介\n",
    "> Machine Leaning is all about learning from something\n",
    "\n",
    "- 关键词：特征（Feature）与标签（Labels）\n",
    "- 有角&无角\n",
    "- 音乐品味"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯上手 Naive Bayes\n",
    "\n",
    "- 散点图：坡度和颠簸度\n",
    "- 从散点图预测\n",
    "- 从散点图到决策面（Decision Surface）\n",
    "- 良好的线性决策面（Data->D.S.)\n",
    "- 朴素贝叶斯是一种常见的寻找决策面的算法。\n",
    "- SKlearn\n",
    "- Gaussian Naive Bayes\n",
    "\n",
    "### 贝叶斯规则\n",
    "\n",
    "> 贝叶斯规则由 Tomas Bayes 发明，他曾用贝叶斯规则推动上帝的存在。\n",
    "\n",
    "**癌症测试**\n",
    "\n",
    "![Bayes rules 图示](img/bayes_rules.jpg)\n",
    "\n",
    "之前的癌症概率是 1%，敏感型和特殊性是 90%，癌症测试结果呈阳性的人患病的概率有多大？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先验和后验。贝叶斯法则是将测试中的证据加入先验概率中，以获得后验概率。\n",
    "- 规范化\n",
    "- 全概率\n",
    "- 贝叶斯规则图\n",
    "- 为何朴素贝叶斯很朴素。将贝叶斯用于文本分类时，未考虑观察到词汇出现的先后顺序，即词序，只是把词出现的频率为依据来进行分类。\n",
    "- 参考文献：\n",
    "    - [浅谈贝叶斯和MCMC](http://www.xuyankun.cn/2017/05/13/bayes/)\n",
    "    - [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)\n",
    "    - [Probabilistic Programming and Bayesian Methods for Hackers](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb)\n",
    "\n",
    "我们知道事件A发生的先验概率$P(A)$（也称之为individual belief），根据观察值$P(B|A)$，来更新事件A发生的后验概率$P(A|B)$。\n",
    "\n",
    "后验概率是概率分布函数，后验概率对事件发生的不确定性由概率分布曲线的宽度决定，换句话说我们对事件发生越相信，概率分布越集中，如下图所示，随着观察值的增多，不断修正后验概率，不确定性也越来越小。\n",
    "\n",
    "![更新后验概率](img/update_posterior_probabilities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝叶斯决策步骤\n",
    "\n",
    "> 贝叶斯决策主要包含四个部分：数据（D）、假设（H）、目标（O）、决策（S）。数据即证据（如前面的癌症案列中测到的阳性、阴性），假设是我们要验证的事实，目标是我们最终要取得优化的量，决策时根据目标得到的最后行为。如果假设空间是连续的，往往以参数空间给出。\n",
    "\n",
    "1. 理清因果关系，哪个是证据（数据），哪个是假设？\n",
    "2. 给出所有可能的假设，即假设空间。\n",
    "3. 给出假设的先验概率。\n",
    "4. 根据贝叶斯概率公式求解后验概率，得到假设空间的后验概率分布。\n",
    "5. 利用后验概率求解条件期望，得到条件期望最大值的行为。\n",
    "\n",
    "假设给你一个人的身高和体重资料， 你不知道他的男女性别，由贝叶斯决策机制解决如下： 首先，此处证据是身高和体重，假设是男或女。先验概率是人口中的男女比例， 而我们需要掌握的条件概率是男性和女性的身高和体重分布。然后根据贝叶斯公式求解后验概率，而此处要做的决策是男女，目标是分类错误率最低，决策即性别分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 朴素贝叶斯迷你项目\n",
    "\n",
    "> 朴素贝叶斯算法非常适合文本分类。在处理文本时，常见的做法是将每个单词看作一个特征，这样就会有大量的特征。朴素贝叶斯之所以朴素，也就是因为把每个单词看作是独立不相关的变量了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 支持向量机 Support Vector Machine\n",
    "\n",
    "- 分割线\n",
    "- 选择分割线\n",
    "- 好的分割线的特点：**分割线在每个分类中均最大化了到最近邻数据点的距离（间隔）**。\n",
    "    - A Line Maximizes Distance To Nearest Point in Either Class.\n",
    "    - Distance to Nearest Point = Margin.\n",
    "    - The inside of the SVM is to maximize robustness of your result.\n",
    "- 对于支持向量机，首先应尽可能地保证分类正确。\n",
    "- SVM对异常值的响应（**软间隔 Soft Margin SVM**）。SVM对异常值处理的情况是较为健壮的，在某种程度上均衡了找出最大间隔距离和忽略异常值的能力。当然这也是有代价的，SVM参数的选取决定了如何检测新的异常值。\n",
    "- SVM 决策边界 Decision Boundary\n",
    "- 非线性 Nonlinear SVM\n",
    "- 创造新特征 $x^2+y^2$\n",
    "![Nonlinear SVM](img/nonlinear_svm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 核技巧 Kernel Trick 本质就是升维\n",
    "    - 将数据特征通过Kernel变换为高维特征空间。\n",
    "    - 在高维特征空间通过SVM找到数据分隔的超平面。\n",
    "    - 在将超平面变换为原空间，变为非线性分隔方案。\n",
    "- sklearn.svm.SVC Support Vector Classification\n",
    "- 尝试各种核：rbf(default), linear, sigmoid, precompute, callable.\n",
    "- SVM的参数\n",
    "    - Kernel\n",
    "    - C: control the tradeoff between smooth decision boundary and classifying training points correctly.\n",
    "    - C值越大，得到更多的正确训练点，相应的的分界点越崎岖。\n",
    "    - Gamma\n",
    "- 过拟合。SVM中$C$、$\\gamma$、Kernel都会影响到SVM过拟合，因此one of the artistry of Machine Learning is tunning parameters.\n",
    "\n",
    "#### 参考文献\n",
    "- [靠靠靠谱给出的SVM思路](https://www.zhihu.com/question/21094489)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 迷你项目\n",
    "\n",
    "> Sebastian 说朴素贝叶斯非常适合文本，是指对于这一具体问题，朴素贝叶斯不仅更快，而且通常比 SVM 更出色。当然，SVM 更适合许多其他问题。当你第一次求解问题时就知道该尝试哪个算法，这是机器学习艺术和科学性的一个体现。除了选择算法外，视你尝试的算法而定，你还需要考虑相应的参数调整以及过拟合的可能性（特别是在你没有大量训练数据的情况下）。\n",
    "\n",
    "> 我们通常建议你尝试一些不同的算法来求解每个问题。调整参数的工作量很大，但你现在只需要听完这堂课，我们将向你介绍 GridCV，一种几乎能自动查找最优参数调整的优秀 sklearn 工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树 Decision Tree D.T.\n",
    "> 决策树可通过简单的线性决策面（linear decision surfaces），做出非线性决策。\n",
    "\n",
    "- 可线性分离的数据\n",
    "- 多远线性问题：Decision trees allow you to ask multiple linear questions, one after another.\n",
    "- 构建决策树/第n(n=1,2,3)次分隔。找出每一步的决策树边界\n",
    "- 决策树编码 Decision Boundry\n",
    "- 决策树准确性 Accuracy\n",
    "- [决策树参数](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "    - 最小样本分隔 min_samples_split 对于决策树的最底层，当样本数=min_samples_split 就不再继续做进一步分割了。\n",
    "- 数据杂质与熵 Entrop, measures of impurity in a bunch of examples。 Controls how a DT decides where to split the data.\n",
    "- 分割中尽可能减少杂质\n",
    "- 熵公式  $entropy = \\sum_i{-p_i log_2(p_i)}$\n",
    "- 信息增益\n",
    "    - Information Gain = entropy(parent)-[weighted average]entropy(children)\n",
    "    - decision tree algorithm: maximize information gain\n",
    "- 信息增益计算\n",
    "- 调整标准参数\n",
    "- 偏差（Bias）& 方差(Variance)困境\n",
    "    - 高偏差机器学习算法实际会忽略数据,几乎没有能力学习任何数据，因此称为偏差（Bias)。\n",
    "    - 方差极高的机器学习算法对数据极度敏感，并且只能复制曾经见过的东西。它的问题在于对曾经未见过的情况下适应度非常差，因为它没有适当的偏差来泛化新东西。\n",
    "    - 实际上我们需要一个中庸的算法，来权衡偏差和方差，以控制机器学习算法具有一定的泛化能力，但同时仍然保持开放，能侦听数据。\n",
    "\n",
    "> The art of tilting the knob to make more bias or high variance, is the art of making machine learning.\n",
    "\n",
    "- 决策树的优缺点\n",
    "    - 优点是直观明了，可以图的形式展示每一步的决策过程。可以集成到更大分类器的集成方法中。\n",
    "    - 缺点是容易过拟合，尤其是在具有包含大量特征的数据时。因此在适当的时间点停止决策树生长（min_samples_split)就显得特别重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-2/3.*math.log(2./3,2)-1/3.*math.log(1./3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树迷你项目\n",
    "> 1、参数调整可以显著加快机器学习算法的训练时间。\n",
    "\n",
    "> 2、控制算法复杂度的另一种方法是通过你在训练/测试时用到的特征数量。算法可用的特征数越多，越有可能发生复杂拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集与问题——安然（Eron）数据分析\n",
    "- 准确率与训练集大小：More data > Fine-tunned algorithm\n",
    "\n",
    "将字典数据转化为pandas的dataframe来完成练习。\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(enron_data).T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 回归 Continuous Supervised Learning\n",
    "\n",
    "- 连续输出\n",
    "- 年龄是连续还是离散？连续\n",
    "- 天气是连续还是离散？离散\n",
    "- 电子邮件作者？离散\n",
    "- 电话号码？离散\n",
    "- 收入？连续\n",
    "- 连续特征\n",
    "- 具有连续输出的监督学习\n",
    "- 回归线性方程（Linear Regression）\n",
    "- 斜率（slope）和截距（intercept）\n",
    "- 使用回归的预测\n",
    "- 编码\n",
    "- sklearn中的线性回归\n",
    "   - `from sklearn.linear_model import LinearRegression`\n",
    "- 通过sklearn提取分数数据\n",
    "    - 斜率 `reg.coef_`\n",
    "    - 截距 `reg.intercept_`\n",
    "    - `R-squared` 数值越高表示线性模型的性能越好。可通过 `Reg.score(feats, labels)` 提取\n",
    "- 线性回归误差 Linear Regression Errors\n",
    "- 误差和拟合质量\n",
    "- 最小化误差平方和的算法\n",
    "    - **Ordinary least squares (OLS)**: used in sklearn LinearRegression\n",
    "    - **Gradient Descent**\n",
    "- 最小化Sum of Squared Errors (SSE)\n",
    "- 最小化绝对误差的问题：**当最优化线的上下部分数据点一样多的时候，会存在多个最优解，即模糊性（Ambiguity）**\n",
    "- 肉眼评估回归\n",
    "- SSE的问题：当数据点数增多，SSE必定会增大，但这不能说明拟合性能不好。\n",
    "- 回归的R平方指标：\n",
    "    - 描述线性回归的好坏指标。\n",
    "    - R-squared越大说明线性拟合度越好。\n",
    "    - $0\\le r^2\\le 1$\n",
    "    - R-squared 以训练集数据的多少无关\n",
    "- sklearn 中的 `R`平方：`reg.score()`\n",
    "- 线性回归的可视化\n",
    "```python\n",
    "plt.scatter(ages, worths)\n",
    "plt.plot(ages, reg.predict(ages), color='blue', linewidth=3)\n",
    "plt.xlabels(\"ages\")\n",
    "plt.ylabels(\"worths\")\n",
    "plt.show()\n",
    "```\n",
    "- 什么数据适用于线性回归\n",
    "- 分类 VS 回归\n",
    "    - 输出类型 discret or continues\n",
    "    - what are you trying to find? decision boundry or \"best fit line\"\n",
    "    - evaluation: accuracy or $r^2$\n",
    "- 多元回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归迷你项目\n",
    "\n",
    "单一的异常值对回归会有很大影响\n",
    "\n",
    "## 异常值\n",
    "\n",
    "### 异常值排除策略\n",
    "\n",
    "1. 用数据集训练模型\n",
    "2. 识别并排除数据集中残差（Linear Residual Error）最大的k（如10%）部分数据\n",
    "3. 重新训练模型\n",
    "4. 重复步骤2、3。\n",
    "\n",
    "## 异常值迷你项目\n",
    "\n",
    "### 异常值清理\n",
    "```python\n",
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        Clean away the 10% of points that have the largest residual errors (difference between the prediction and the actual net worth).\n",
    "        Return a list of tuples named cleaned_data where each tuple is of the form (age, net_worth, error).\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "\n",
    "    ### your code goes here\n",
    "    for prediction, age, worth in zip(predictions, ages, net_worths):\n",
    "        error = abs(prediction-worth)\n",
    "        cleaned_data.append((age, worth, error))\n",
    "    # sort the cleaned data by error\n",
    "    cleaned_data.sort(key=lambda x: x[2])\n",
    "    # retain the first 90% of data\n",
    "    n = int(0.9*len(cleaned_data))\n",
    "    return cleaned_data[0:n]\n",
    "```\n",
    "\n",
    "### 安然异常值\n",
    "\n",
    "> 可视化是查找异常值最强大的工具之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50/8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
