{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习基础\n",
    "## 什么是机器学习\n",
    "- 决策树 Decision Tree: APP推荐\n",
    "- 朴素贝叶斯 Naive Bayes Rule: 垃圾邮件判别\n",
    "- 梯度下降 Gradient Decent\n",
    "- 线性回归 Linear Regression: 房价预测 连续值\n",
    "- 对数几率回归 Logistic Regression: 学生录取 离散值 分类问题\n",
    "- 支持向量机 Support Vector Machine\n",
    "- 核函数 Kernel Trick: 用于SVM；用一条曲线分类数据或者数据转化到高维空间用平面划分。\n",
    "- 神经网络 Neural Network: 学生录取\n",
    "- K均值聚类 K-means: 面包店选址\n",
    "- 层次聚类 Hierarchical Clustering: 一种解决K-means算法中k难以确定的聚类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置Anaconda\n",
    "- Anaconda[下载网址](https://www.continuum.io/downloads)\n",
    "- 本次安装的python 3.6 version Graphical Installer\n",
    "- 安装后不能直接在命令行使用`conda`指令，需在.bashrc加入`export PATH=\"/<path to anaconda>/bin:$PATH\"`，详情参考Anaconda[安装指南](https://docs.continuum.io/anaconda/install/mac-os#macos-graphical-install)\n",
    "- 常用conda指令：\n",
    "    - 当前环境包管理信息 `conda list`\n",
    "    - 创建环境 `conda create -n my_env python=2`\n",
    "    - 已有环境列表 `conda env list`\n",
    "    - 进入环境（Mac) `source activate my_env`\n",
    "    - 退出环境 `deactivate`\n",
    "    - 安装包 `conda install pkg/like/numpy`\n",
    "    - conda帮助 `conda -h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置Jupyter notebook\n",
    "- 在当前环境安装jupyter notebook `conda install jupyter notebook`\n",
    "- 启动 notebook 服务器，在终端或控制台中输入 jupyter notebook\n",
    "- 命令行模式：单元格为蓝色；编辑模式：单元格为绿色\n",
    "- 命令行模式下神器 comand platte 呼出快捷键 `cmd/ctrl+shift+p` 或者直接 `p`\n",
    "- 命令行模式下查看快捷键列表：`h`\n",
    "- Magic 关键字是可以在单元格中运行的特殊命令。例如，在 notebook 中可以使用 %matplotlib 将 matplotlib 设置为以交互方式工作\n",
    "- Magic 命令的前面带有一个或两个百分号（% 或 %%），分别对应行 Magic 命令和单元格 Magic 命令。行 Magic 命令仅应用于编写 Magic 命令时所在的行，而单元格 Magic 命令应用于整个单元格\n",
    "- 代码计时 %timeit %%timeit\n",
    "- 更多[Magic列表](http://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "- 支持数学公式，如：$a=\\frac{b}{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的中心：众数，平均数和中位数\n",
    "### 众数 Mode\n",
    "> 定义：一组数据中出现次数最多的数值，众数可以不存在也可以多于一个。\n",
    "\n",
    "- 数据集中不是所有的分值都会影响众数\n",
    "- 从统一分布中抽取出的一组样本，其众数不一定相同，且众数的取值受分组大小（bin)的影响。\n",
    "- 众数没有一个计算公式\n",
    "\n",
    "### 平均数 Mean\n",
    "**定义**：\n",
    "\n",
    "采样中记为：$$\\overline{x}=\\frac{\\sum_{i=1}^{n}{x_i}}{n}$$\n",
    "\n",
    "分布中记为：$$\\mu=\\frac{\\sum{x}}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "geo = [48670, 57320, 38150, 41290, 53160,500000]\n",
    "mean_geo = np.sum(geo)/len(geo)\n",
    "print(mean_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均数特性：\n",
    "\n",
    "- 分布中的所有分值都影响平均值\n",
    "- 平均值可用公式来描述\n",
    "- 同一个总体中的多个样本会有相似的平均值\n",
    "- 一个样本的平均值可以用来推论其所在的总体\n",
    "- 如果向数据集中添加一个极值，它的平均值会发生改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![平均数示意](img/mean.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "含异常（outlier）的均值，不能良好的评估数据集。一个有趣的例子是，北卡莱罗纳大学地理专业毕业生的平均薪资远高于该专业其他大学的毕业生，原因竟然是因为北卡莱罗纳大学地理专业毕业生中有一位名叫乔丹的明星，他的个人薪资极大地拉高了该专业的平均薪资。因此异常值的出现会影响用平均值评估数据集。\n",
    "\n",
    "这就是为什么需要引入中位数。**中位数受异常值影响较小。**\n",
    "\n",
    "### 中位数 Median\n",
    "> 中位数是位于“中间”的数据，意味着有一半数据值小于它，而另一半大于它。\n",
    "\n",
    "[婚庆行业的价格秘密](http://www.slate.com/articles/life/weddings/2013/06/average_wedding_cost_published_numbers_on_the_price_of_a_wedding_are_totally.html)举例说明了知道何时和如何使用中位数及均值，是多么重要。这个例子和婚礼费用有关，向你展示了为什么平均婚礼费用不适用于说明普通夫妇的婚礼开支。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     241\n",
       "2    1116\n",
       "3      69\n",
       "4    1214\n",
       "Name: friends, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fb_friends = pd.read_csv(\"data/How_many_Facebook_friends_do_Udacians_have.csv\")\n",
    "fb_friends['friends'].median()\n",
    "fb_friends.iloc[1:5,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的差异性：值域，IQR，方差和标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 量化数据的分布形态\n",
    "- 最大值和最小值之间的差（值域）\n",
    "- 当异常值出现时，值域不能良好的代表数据的差异性\n",
    "\n",
    "#### 砍掉尾巴\n",
    "虽然 Katie 建议去除上下各 25% 的数据点，但我们在去除数据点时还需要特别谨慎，特别是在数据量不大的情况下，去除一半的数据点会让我们丢失大量数据的信息。\n",
    "\n",
    "一般来说，在去除数据点前，我们建议首先将数据点通过图像表述出来（直方图、散点图、箱线图等），这样可以帮助你获得对数据整理分析的了解。然后，基于项目的背景，判断你更关心数据的哪一部分（大多数正常数据，还是小部分异常数据），因为在一些项目背景下，你可能更关心那些异常值，比如在 数据分析（进阶）课程的安然数据分析中，那些工资异常高的人更有可能腐败。最后，是基于现有的数据量作出决定，究竟是直接丢弃部分数据还是对部分作出调整，亦或是有保留地接受所有数据。特别记住一点，没有一种分析方法100%正确，但我们总可以尝试根据不同的需求找到一种最合理的方法。\n",
    "\n",
    "#### IQR = Q3-Q1 Interquartile range\n",
    "第三个四分之一减去第一个四分之一\n",
    "- 几乎 50% 的数据在 IQR 间\n",
    "- IQR 不受异常值的影响\n",
    "\n",
    "#### 异常值 Outlier\n",
    "$$ Outlier < Q_1 - 1.5IQR $$\n",
    "$$ Outlier > Q_3 + 1.5IQR $$\n",
    "\n",
    "#### 箱线图 box and whisker\n",
    "![箱线图](img/boxandwhisker.jpg)\n",
    "\n",
    "**问**：均值一定在IRQ中吗？\n",
    "\n",
    "**答**：No，均值受Outlier影响严重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 衡量差异性的方法\n",
    "- 找出每个值与数据集均值之间差的平均值\n",
    "- 正偏差和负偏差相互抵消\n",
    "- 绝对偏差\n",
    "\n",
    "**平均绝对偏差**：\n",
    "\n",
    "$$ \\frac{\\sum{|x_i-\\overline{x}|}}{n}$$\n",
    "\n",
    "**SS Sum of Squre 平方和**：\n",
    "\n",
    "$$ \\sum{(x_i-\\overline{x}})^2$$\n",
    "\n",
    "**方差 Variance**：\n",
    "\n",
    "$$ \\frac{\\sum{(x_i-\\overline{x}})^2}{n}$$\n",
    "\n",
    "![方差](img/variance.jpg)\n",
    "\n",
    "**标准偏差 Standard Deviation**：\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$\n",
    "\n",
    "![标准偏差](img/standard_deviation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝塞尔校正\n",
    "我们发现从采样样本中计算出的偏差一般比原始数据小。这是因为在正态分布中，采样数据倾向于取峰值附近的数据，因此采样数据的偏差比原始数据小得多，如下图所示：\n",
    "![Bessel Correction](img/bessel_correction.jpg)\n",
    "\n",
    "为了矫正以上误差，我们一般用贝塞尔矫正修正偏差公式。这就是为什么以前书上写的标准差公式的分母为n-1的原因。\n",
    "**样本标准方差**：\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n-1}}$$\n",
    "\n",
    "**分布标准方差**：\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$\n",
    "\n",
    "#### 澄清样本SD值的真正含义\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n-1}} \\approx \\sigma = \\sqrt{\\frac{\\sum{(x_i-\\overline{x}})^2}{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 监督学习简介\n",
    "> Machine Leaning is all about learning from something\n",
    "\n",
    "- 关键词：特征（Feature）与标签（Labels）\n",
    "- 有角&无角\n",
    "- 音乐品味"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯上手 Naive Bayes\n",
    "\n",
    "- 散点图：坡度和颠簸度\n",
    "- 从散点图预测\n",
    "- 从散点图到决策面（Decision Surface）\n",
    "- 良好的线性决策面（Data->D.S.)\n",
    "- 朴素贝叶斯是一种常见的寻找决策面的算法。\n",
    "- SKlearn\n",
    "- Gaussian Naive Bayes\n",
    "\n",
    "### 贝叶斯规则\n",
    "\n",
    "> 贝叶斯规则由 Tomas Bayes 发明，他曾用贝叶斯规则推动上帝的存在。\n",
    "\n",
    "**癌症测试**\n",
    "\n",
    "![Bayes rules 图示](img/bayes_rules.jpg)\n",
    "\n",
    "之前的癌症概率是 1%，敏感型和特殊性是 90%，癌症测试结果呈阳性的人患病的概率有多大？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先验和后验。贝叶斯法则是将测试中的证据加入先验概率中，以获得后验概率。\n",
    "- 规范化\n",
    "- 全概率\n",
    "- 贝叶斯规则图\n",
    "- 为何朴素贝叶斯很朴素。将贝叶斯用于文本分类时，未考虑观察到词汇出现的先后顺序，即词序，只是把词出现的频率为依据来进行分类。\n",
    "- 参考文献：\n",
    "    - [浅谈贝叶斯和MCMC](http://www.xuyankun.cn/2017/05/13/bayes/)\n",
    "    - [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)\n",
    "    - [Probabilistic Programming and Bayesian Methods for Hackers](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb)\n",
    "\n",
    "我们知道事件A发生的先验概率$P(A)$（也称之为individual belief），根据观察值$P(B|A)$，来更新事件A发生的后验概率$P(A|B)$。\n",
    "\n",
    "后验概率是概率分布函数，后验概率对事件发生的不确定性由概率分布曲线的宽度决定，换句话说我们对事件发生越相信，概率分布越集中，如下图所示，随着观察值的增多，不断修正后验概率，不确定性也越来越小。\n",
    "\n",
    "![更新后验概率](img/update_posterior_probabilities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝叶斯决策步骤\n",
    "\n",
    "> 贝叶斯决策主要包含四个部分：数据（D）、假设（H）、目标（O）、决策（S）。数据即证据（如前面的癌症案列中测到的阳性、阴性），假设是我们要验证的事实，目标是我们最终要取得优化的量，决策时根据目标得到的最后行为。如果假设空间是连续的，往往以参数空间给出。\n",
    "\n",
    "1. 理清因果关系，哪个是证据（数据），哪个是假设？\n",
    "2. 给出所有可能的假设，即假设空间。\n",
    "3. 给出假设的先验概率。\n",
    "4. 根据贝叶斯概率公式求解后验概率，得到假设空间的后验概率分布。\n",
    "5. 利用后验概率求解条件期望，得到条件期望最大值的行为。\n",
    "\n",
    "假设给你一个人的身高和体重资料， 你不知道他的男女性别，由贝叶斯决策机制解决如下： 首先，此处证据是身高和体重，假设是男或女。先验概率是人口中的男女比例， 而我们需要掌握的条件概率是男性和女性的身高和体重分布。然后根据贝叶斯公式求解后验概率，而此处要做的决策是男女，目标是分类错误率最低，决策即性别分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 朴素贝叶斯迷你项目\n",
    "\n",
    "> 朴素贝叶斯算法非常适合文本分类。在处理文本时，常见的做法是将每个单词看作一个特征，这样就会有大量的特征。朴素贝叶斯之所以朴素，也就是因为把每个单词看作是独立不相关的变量了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 支持向量机 Support Vector Machine\n",
    "\n",
    "- 分割线\n",
    "- 选择分割线\n",
    "- 好的分割线的特点：**分割线在每个分类中均最大化了到最近邻数据点的距离（间隔）**。\n",
    "    - A Line Maximizes Distance To Nearest Point in Either Class.\n",
    "    - Distance to Nearest Point = Margin.\n",
    "    - The inside of the SVM is to maximize robustness of your result.\n",
    "- 对于支持向量机，首先应尽可能地保证分类正确。\n",
    "- SVM对异常值的响应（**软间隔 Soft Margin SVM**）。SVM对异常值处理的情况是较为健壮的，在某种程度上均衡了找出最大间隔距离和忽略异常值的能力。当然这也是有代价的，SVM参数的选取决定了如何检测新的异常值。\n",
    "- SVM 决策边界 Decision Boundary\n",
    "- 非线性 Nonlinear SVM\n",
    "- 创造新特征 $x^2+y^2$\n",
    "![Nonlinear SVM](img/nonlinear_svm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 核技巧 Kernel Trick 本质就是升维\n",
    "    - 将数据特征通过Kernel变换为高维特征空间。\n",
    "    - 在高维特征空间通过SVM找到数据分隔的超平面。\n",
    "    - 在将超平面变换为原空间，变为非线性分隔方案。\n",
    "- sklearn.svm.SVC Support Vector Classification\n",
    "- 尝试各种核：rbf(default), linear, sigmoid, precompute, callable.\n",
    "- SVM的参数\n",
    "    - Kernel\n",
    "    - C: control the tradeoff between smooth decision boundary and classifying training points correctly.\n",
    "    - C值越大，得到更多的正确训练点，相应的的分界点越崎岖。\n",
    "    - Gamma\n",
    "- 过拟合。SVM中$C$、$\\gamma$、Kernel都会影响到SVM过拟合，因此one of the artistry of Machine Learning is tunning parameters.\n",
    "\n",
    "#### 参考文献\n",
    "- [靠靠靠谱给出的SVM思路](https://www.zhihu.com/question/21094489)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 迷你项目\n",
    "\n",
    "> Sebastian 说朴素贝叶斯非常适合文本，是指对于这一具体问题，朴素贝叶斯不仅更快，而且通常比 SVM 更出色。当然，SVM 更适合许多其他问题。当你第一次求解问题时就知道该尝试哪个算法，这是机器学习艺术和科学性的一个体现。除了选择算法外，视你尝试的算法而定，你还需要考虑相应的参数调整以及过拟合的可能性（特别是在你没有大量训练数据的情况下）。\n",
    "\n",
    "> 我们通常建议你尝试一些不同的算法来求解每个问题。调整参数的工作量很大，但你现在只需要听完这堂课，我们将向你介绍 GridCV，一种几乎能自动查找最优参数调整的优秀 sklearn 工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树 Decision Tree D.T.\n",
    "> 决策树可通过简单的线性决策面（linear decision surfaces），做出非线性决策。\n",
    "\n",
    "- 可线性分离的数据\n",
    "- 多远线性问题：Decision trees allow you to ask multiple linear questions, one after another.\n",
    "- 构建决策树/第n(n=1,2,3)次分隔。找出每一步的决策树边界\n",
    "- 决策树编码 Decision Boundry\n",
    "- 决策树准确性 Accuracy\n",
    "- [决策树参数](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "    - 最小样本分隔 min_samples_split 对于决策树的最底层，当样本数=min_samples_split 就不再继续做进一步分割了。\n",
    "- 数据杂质与熵 Entrop, measures of impurity in a bunch of examples。 Controls how a DT decides where to split the data.\n",
    "- 分割中尽可能减少杂质\n",
    "- 熵公式  $entropy = \\sum_i{-p_i log_2(p_i)}$\n",
    "- 信息增益\n",
    "    - Information Gain = entropy(parent)-[weighted average]entropy(children)\n",
    "    - decision tree algorithm: maximize information gain\n",
    "- 信息增益计算\n",
    "- 调整标准参数\n",
    "- 偏差（Bias）& 方差(Variance)困境\n",
    "    - 高偏差机器学习算法实际会忽略数据,几乎没有能力学习任何数据，因此称为偏差（Bias)。\n",
    "    - 方差极高的机器学习算法对数据极度敏感，并且只能复制曾经见过的东西。它的问题在于对曾经未见过的情况下适应度非常差，因为它没有适当的偏差来泛化新东西。\n",
    "    - 实际上我们需要一个中庸的算法，来权衡偏差和方差，以控制机器学习算法具有一定的泛化能力，但同时仍然保持开放，能侦听数据。\n",
    "\n",
    "> The art of tilting the knob to make more bias or high variance, is the art of making machine learning.\n",
    "\n",
    "- 决策树的优缺点\n",
    "    - 优点是直观明了，可以图的形式展示每一步的决策过程。可以集成到更大分类器的集成方法中。\n",
    "    - 缺点是容易过拟合，尤其是在具有包含大量特征的数据时。因此在适当的时间点停止决策树生长（min_samples_split)就显得特别重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树迷你项目\n",
    "> 1、参数调整可以显著加快机器学习算法的训练时间。\n",
    "\n",
    "> 2、控制算法复杂度的另一种方法是通过你在训练/测试时用到的特征数量。算法可用的特征数越多，越有可能发生复杂拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集与问题——安然（Eron）数据分析\n",
    "- 准确率与训练集大小：More data > Fine-tunned algorithm\n",
    "\n",
    "将字典数据转化为pandas的dataframe来完成练习。\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(enron_data).T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 回归 Continuous Supervised Learning\n",
    "\n",
    "- 连续输出\n",
    "- 年龄是连续还是离散？连续\n",
    "- 天气是连续还是离散？离散\n",
    "- 电子邮件作者？离散\n",
    "- 电话号码？离散\n",
    "- 收入？连续\n",
    "- 连续特征\n",
    "- 具有连续输出的监督学习\n",
    "- 回归线性方程（Linear Regression）\n",
    "- 斜率（slope）和截距（intercept）\n",
    "- 使用回归的预测\n",
    "- 编码\n",
    "- sklearn中的线性回归\n",
    "   - `from sklearn.linear_model import LinearRegression`\n",
    "- 通过sklearn提取分数数据\n",
    "    - 斜率 `reg.coef_`\n",
    "    - 截距 `reg.intercept_`\n",
    "    - `R-squared` 数值越高表示线性模型的性能越好。可通过 `Reg.score(feats, labels)` 提取\n",
    "- 线性回归误差 Linear Regression Errors\n",
    "- 误差和拟合质量\n",
    "- 最小化误差平方和的算法\n",
    "    - **Ordinary least squares (OLS)**: used in sklearn LinearRegression\n",
    "    - **Gradient Descent**\n",
    "- 最小化Sum of Squared Errors (SSE)\n",
    "- 最小化绝对误差的问题：**当最优化线的上下部分数据点一样多的时候，会存在多个最优解，即模糊性（Ambiguity）**\n",
    "- 肉眼评估回归\n",
    "- SSE的问题：当数据点数增多，SSE必定会增大，但这不能说明拟合性能不好。\n",
    "- 回归的R平方指标：\n",
    "    - 描述线性回归的好坏指标。\n",
    "    - R-squared越大说明线性拟合度越好。\n",
    "    - $0\\le r^2\\le 1$\n",
    "    - R-squared 以训练集数据的多少无关\n",
    "- sklearn 中的 `R`平方：`reg.score()`\n",
    "- 线性回归的可视化\n",
    "```python\n",
    "plt.scatter(ages, worths)\n",
    "plt.plot(ages, reg.predict(ages), color='blue', linewidth=3)\n",
    "plt.xlabels(\"ages\")\n",
    "plt.ylabels(\"worths\")\n",
    "plt.show()\n",
    "```\n",
    "- 什么数据适用于线性回归\n",
    "- 分类 VS 回归\n",
    "    - 输出类型 discret or continues\n",
    "    - what are you trying to find? decision boundry or \"best fit line\"\n",
    "    - evaluation: accuracy or $r^2$\n",
    "- 多元回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归迷你项目\n",
    "\n",
    "单一的异常值对回归会有很大影响\n",
    "\n",
    "## 异常值\n",
    "\n",
    "### 异常值排除策略\n",
    "\n",
    "1. 用数据集训练模型\n",
    "2. 识别并排除数据集中残差（Linear Residual Error）最大的k（如10%）部分数据\n",
    "3. 重新训练模型\n",
    "4. 重复步骤2、3。\n",
    "\n",
    "## 异常值迷你项目\n",
    "\n",
    "### 异常值清理\n",
    "```python\n",
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        Clean away the 10% of points that have the largest residual errors (difference between the prediction and the actual net worth).\n",
    "        Return a list of tuples named cleaned_data where each tuple is of the form (age, net_worth, error).\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "\n",
    "    ### your code goes here\n",
    "    for prediction, age, worth in zip(predictions, ages, net_worths):\n",
    "        error = abs(prediction-worth)\n",
    "        cleaned_data.append((age, worth, error))\n",
    "    # sort the cleaned data by error\n",
    "    cleaned_data.sort(key=lambda x: x[2])\n",
    "    # retain the first 90% of data\n",
    "    n = int(0.9*len(cleaned_data))\n",
    "    return cleaned_data[0:n]\n",
    "```\n",
    "\n",
    "### 安然异常值\n",
    "\n",
    "> 可视化是查找异常值最强大的工具之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 交叉验证 Cross Validation\n",
    "\n",
    "### 测试的好处 Why using training and testing data?\n",
    "\n",
    "- Give estimate of performance on an independent dataset\n",
    "- serve as check on overfitting\n",
    "\n",
    "### 在Sklearn中训练/测试分离\n",
    "\n",
    "```python\n",
    "# for version v1.7\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# for version v1.8\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_split(feats, labels, test_size = 0.2, random_state = 0)\n",
    "```\n",
    "\n",
    "### K折交叉验证 K-fold Cross Validation\n",
    "\n",
    "对于训练数据集，我们自然希望更多的数据充当训练集，因为这样可以训练获得更好的学习效果，更多的训练集意味着更少的验证集，但我们仍期望更多的数据充当验证集以获得最佳的验证。因此就存在一个折中方案，以平衡训练集和验证集。\n",
    "\n",
    "于是就提出了**交叉验证**，我们把数据集平均分为$k=10$部分，每次随机取其中的1份作为验证集，其余部分为测试集，如此独立重复$k=10$次学习试验，最后对这$k=10$次学习试验效果进行平均。\n",
    "\n",
    "- 相对简单的训练/测试方法，交叉验证更耗时。\n",
    "- 优点是，学习的评估效果更佳准确，因为这种方法几乎使用了所有数据作为训练集同时几乎使用了所有数据做验证集。\n",
    "\n",
    "### Sklearn 中的 K-fold C.V.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "KFold(n_elements, k_fold)\n",
    "```\n",
    "\n",
    "### 针对Sklearn中 K-fold 的实用建议\n",
    "\n",
    "k-fold均分时应采用随机均分\n",
    "```python\n",
    "KFold(n_elements, k_fold, shuffle=True)\n",
    "```\n",
    "\n",
    "### 为调整参数而进行交叉验证\n",
    "\n",
    "> 交叉验证可通过多次试验，选择实现最佳性能的参数调整方式。\n",
    "\n",
    "### Sklearn 中的 GridSearchCV\n",
    "\n",
    "> GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。它的好处是，只需增加几行代码，就能遍历多种组合。\n",
    "\n",
    "```python\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "# 下面行可理解为：“分类器”在这种情况下不仅仅是一个算法，而是算法加参数值。\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "# 拟合函数现在尝试了所有的参数组合，并返回一个合适的分类器，自动调整至最佳参数组合。\n",
    "# 可通过 clf.best_params_ clf.best_estimator_ 来获得参数值。\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证迷你项目\n",
    "\n",
    "## 评估指标 Evaluation Metrics\n",
    "\n",
    "- 选择合适的指标\n",
    "\n",
    "> 在构建机器学习模型时，我们首先要选择性能指标，然后测试模型的表现如何。相关的指标有多个，具体取决于我们要尝试解决的问题。\n",
    "\n",
    "- 分类和回归\n",
    "- 分类与回归指标\n",
    "    - 对于分类，关注准确率、精确率、召回率和 F 分数。\n",
    "    - 对于回归，关注平均绝对误差和均方误差。\n",
    "- 分类指标。测量预测是否准确地将所讨论的实例进行分类。\n",
    "- 准确率（Accuracy）。准确率实际上是所有被正确标示的数据点除以所有的标识数据点。\n",
    "- 准确率的缺陷。\n",
    "- 选择合适的指标\n",
    "- 混淆矩阵 Confuse Matrix\n",
    "- 决策树的混淆矩阵\n",
    "![confuse matrix](img/confuse_matrix.jpg)\n",
    "\n",
    "### 精确率(Precision)和召回率/查全率(Recall)\n",
    "\n",
    "- 以猫狗识别为例，精确率是指识别为猫的样本中真正为猫的样本的比例。\n",
    "- 查全率是指查出来真正为猫的样本占了猫样本总数的比率（包含未查出来的）。\n",
    "- 精确率主要用于虚警成本很高的场景，期望识别出的对象尽可能正确。精确率标志识别出来的样本中，有多少比例是正确的（true positive）。\n",
    "- 查全率主要用于”宁错杀也不放过“的场景，期望把所有潜在对象都识别出来。如所有样本中猫的总数为$n$,识别出来$m$只，查全率为$m/n$。\n",
    "- $precesion = (True Positive)/(True Positive + False Positive)$\n",
    "- $recall = (True Positive)/(True Positive + False Negative)$\n",
    "    \n",
    "### F1分数\n",
    "- F1 分数同时考虑精确率和召回率，以计算新的分数\n",
    "-  F1 分数可理解为精确率和召回率的加权平均值，其中 F1 分数的最佳值为 1、最差值为 0\n",
    "- F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)\n",
    "- `sklearn.metrics.f1_score(y_true, y_pred)`\n",
    "    \n",
    "### 回归指标\n",
    "- 对于连续数据预测的模型，我们更关注预测的接近程度。\n",
    "- 例如，对于身高和体重预测，我们不是很关心模型能否将某人的体重 100% 准确地预测到小于零点几磅，但可能很关心模型如何能始终进行接近的预测（可能与个人的真实体重相差 3-4 磅）。\n",
    "\n",
    "### 平均绝对误差\n",
    "- 平均绝对误差的计算方法是，将各个样本的绝对误差汇总，然后根据数据点数量求出平均误差。\n",
    "- 通过将模型的所有绝对值加起来，可以避免因预测值比真实值过高或过低而抵销误差，并能获得用于评估模型的整体误差指标。\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "```\n",
    "\n",
    "### 均方误差\n",
    "\n",
    "均方误差是另一个经常用于测量模型性能的指标。与绝对误差相比，残差（预测值与真实值的差值）被求平方。\n",
    "\n",
    "对残差求平方的一些好处是：\n",
    "- 自动将所有误差转换为正数\n",
    "- **注重较大的误差而不是较小的误差**\n",
    "- 在微积分中是可微的（可让我们找到最小值和最大值）\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "```\n",
    "\n",
    "### 回归分数函数\n",
    "\n",
    "除了误差指标之外，scikit-learn还包括了两个分数指标，范围通常从0到1，值0为坏，而值1为最好的表现，看起来和分类指标`F分数`类似，都是数字越接近1.0分数就越好。\n",
    "\n",
    "#### $R^2$ 分数\n",
    "\n",
    "$R^2$ 分数，用来计算真值预测的可决系数。在 scikit-learn 里，这也是回归学习器默认的分数方法。\n",
    "\n",
    "> The r2_score function computes R², the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "```\n",
    "\n",
    "- [可汗学院参考资料](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/r-squared-or-coefficient-of-determination)\n",
    "- [skearn对于常见的模型表现衡量方法也有详细的介绍。](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "#### 可释方差分数\n",
    "\n",
    "> Explained variance regression score function. Best possible score is 1.0, lower values are worse.\n",
    "\n",
    "```python\n",
    "sklearn.metrics.explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标 迷你项目\n",
    "```python\n",
    "pred = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "real = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "cnt = 0\n",
    "for p, r in zip(pred, real):\n",
    "    if r == 1 and p == 1:\n",
    "        cnt += 1\n",
    "print(\"true positive\", cnt)\n",
    "\n",
    "cnt = 0\n",
    "for p, r in zip(pred, real):\n",
    "    if r == 0 and p == 1:\n",
    "        cnt += 1\n",
    "print(\"false positive\", cnt)\n",
    "\n",
    "cnt = 0\n",
    "for p, r in zip(pred, real):\n",
    "    if r == 1 and p == 0:\n",
    "        cnt += 1\n",
    "print(\"false negative\", cnt)\n",
    "```\n",
    "\n",
    "### 理解指标\n",
    "\n",
    "> 以人脸识别器为例，当识别器有很高的**查全率**，但没有较高的**精确率**。这意味着，几乎每次测试集中出现 POI (Person Of Interest) 时，识别器都能找出他/她。但这么做的代价是有时会得到 false positives（虚警），这时非 POI 会被错误地标记为 POI。\n",
    "\n",
    "> 当识别器有很高的**精确率**，但没有较高的**查全率**。这意味着每当测试集中 POI 被标记时，将很确定它是真的 POI， 而非虚警。另一方面，这样做的代价是有时会错过真 POI（导致false negative增加），因为实际上**避免了在边缘情况下触发标记。**\n",
    "\n",
    "> 识别器有非常好的 $F_1$ 分数。这是两个世界（查全率和精确率）中最好的识别符，false positive 和 false negative 率均为低，这意味着可以可靠、准确地识别 POI。如果识别器发现了 POI，那么此人几乎可以肯定是 POI，而且如果识别器没有标记某人，那么几乎可以肯定他们不是 POI。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差原因\n",
    "\n",
    "在模型预测中，模型可能出现的误差来自两个主要来源，即：因模型无法表示基本数据的复杂度而造成的**偏差（bias）**，或者因模型对训练它所用的有限数据过度敏感而造成的**方差（variance）**。\n",
    "\n",
    "![偏差和方差](img/bias_variance.jpg)\n",
    "\n",
    "### 偏差造成的误差 - 准确率和欠拟合\n",
    "\n",
    "> 如果模型具有足够的数据，但**因不够复杂**而无法捕捉基本关系，则会出现偏差。这样一来，模型一直会系统地错误表示数据，从而导致准确率降低。这种现象叫做欠拟合（underfitting）。\n",
    "\n",
    "简单来说，如果模型不适当，就会出现偏差。举个例子：如果对象是按颜色和形状分类的，但模型只能按颜色来区分对象和将对象分类（模型过度简化），因而一直会错误地分类对象。\n",
    "\n",
    "或者，我们可能有本质上是多项式的连续数据，但模型只能表示线性关系。在此情况下，我们向模型提供多少数据并不重要，因为模型根本无法表示其中的基本关系，我们需要更复杂的模型。\n",
    "\n",
    "### 方差造成的误差 - 精度和过拟合\n",
    "\n",
    "在训练模型时，通常使用来自较大训练集的有限数量样本。如果利用随机选择的数据子集反复训练模型，可以预料它的预测结果会因提供给它的具体样本而异。在这里，**方差（variance）**用来测量预测结果对于任何给定的测试样本会出现多大的变化。\n",
    "\n",
    "> 出现方差是正常的，但方差过高表明模型无法将其预测结果泛化到更多的数据。对训练集高度敏感也称为过拟合（overfitting），而且通常**出现在模型过于复杂**或我们没有足够的数据支持它时。\n",
    "\n",
    "通常，可以利用更多数据进行训练，以降低模型预测结果的方差并提高精度。如果没有更多的数据可以用于训练，还可以通过限制模型的复杂度来降低方差。\n",
    "\n",
    "### 学习曲线\n",
    "\n",
    "学习曲线用于辨别模型表现的好坏。\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "```\n",
    "其中，estimator是我们正在用来预测的模型，例如它可以是GaussianNB()，X和y是特征和目标。cv是交叉验证生成器，例如KFold()，'n_jobs'是平行运算的参数，train_sizes是多少数量的训练数据用来生成曲线。\n",
    "\n",
    "### Noisy Data, Complex Model\n",
    "\n",
    "### 改进模型的有效性\n",
    "\n",
    "> 可以看到，在给定一组固定数据时，模型不能过于简单或复杂。如果过于简单，模型无法了解数据并会错误地表示数据。但是，如果建立非常复杂的模型，则需要更多数据才能了解基本关系，否则十分常见的是，模型会推断出在数据中实际上并不存在的关系。\n",
    "\n",
    "> 关键在于，通过找出正确的模型复杂度来找到最大限度降低偏差和方差的最有效点。当然，数据越多，模型随着时间推移会变得越好。\n",
    "\n",
    "![bias and variance tradeoff](img/bias_variance_tradeoff.jpg)\n",
    "\n",
    "#### 参考文献\n",
    "- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "- [维基百科](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "### 偏差、方差和特征数量 Bias-Variance Dilemma\n",
    "\n",
    "- 高偏差：对训练集数据的模式关注不够，从数据学到的经验很少，模型过于简化。在训练集中表现为高误差。\n",
    "- 高方差：过分关注训练集数据的模式，模型泛化能力较弱，相当于只是背诵了学习的数据，遇到新数据不能很好的预测。表现为在测试集上高偏差。\n",
    "\n",
    "#### 高偏差\n",
    "\n",
    "当数据需要多个特征来描述数据的模式时，而模型只用到特征中的少数几个（few features），表现为对数据状态关注不够，这属于模型过度简化（oversimplified）的情况，即高偏差情形。\n",
    "\n",
    "#### 高方差\n",
    "\n",
    "当使用非常多(too many)的特征来获得拟合程度最好的回归（carefully minimized SSE, lowest SSE），则容易出现高方差情形。\n",
    "\n",
    "因此在拟合的优度和简单性之间存在某种折中，即为实现拟合优度所需的特征个数。也就是希望使用尽可能少的几个特征来较好拟合训练数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习曲线与模型复杂度\n",
    "\n",
    "### 学习曲线\n",
    "\n",
    "> 机器学习中的学习曲线是一种可视化图形，能根据一系列训练实例中的训练和测试数据比较模型的指标性能。\n",
    "\n",
    "在查看数据与误差之间的关系时，我们通常会看到，随着训练点数量的增加，误差会趋于下降。由于我们尝试构建从经验中学习的模型，因此这很有意义。\n",
    "\n",
    "**我们将训练集和测试集分隔开，以便更好地了解能否将模型泛化到未见过的数据而不是拟合到刚见过的数据。**\n",
    "\n",
    "> 在学习曲线中，当训练曲线和测试曲线均达到稳定阶段，并且两者之间的差距不再变化时，则可以确认模型已尽其所能地了解数据了。\n",
    "\n",
    "#### 偏差\n",
    "\n",
    "当训练误差和测试误差收敛并且相当高时，这实质上表示模型具有偏差。无论我们向其提供多少数据，模型都无法表示基本关系，因而出现系统性的高误差。\n",
    "\n",
    "#### 方差\n",
    "\n",
    "如果训练误差与测试误差之间的差距很大，这实质上表示模型具有高方差。与偏差模型不同的是，如果有更多可供学习的数据，或者能简化表示数据的最重要特征的模型，则通常可以改进具有方差的模型。\n",
    "\n",
    "#### 理想的学习曲线\n",
    "\n",
    "> 模型的最终目标是，误差小并能很好地泛化到未见过的数据（测试数据）。如果测试曲线和训练曲线均收敛，并且误差极低，就能看到这种模型。这种模型能根据未见过的数据非常准确地进行预测。\n",
    "\n",
    "### 模型的复杂度\n",
    "\n",
    "> 与学习曲线图形不同，模型复杂度图形呈现的是模型复杂度如何改变训练曲线和测试曲线，而不是用以训练模型的数据点的数量。一般趋势是，随着模型增大，模型对固定的一组数据表现出更高的变化性(更敏感)。\n",
    "\n",
    "### 学习曲线与模型复杂度\n",
    "\n",
    "> 如果我们获取具有同一组固定数据的相同机器学习算法的学习曲线，但为越来越高的模型复杂度创建几个图形，则所有学习曲线图形均代表模型复杂度图形。这就是说，如果我们获取了每个模型复杂度的最终测试误差和训练误差，并依据模型复杂度将它们可视化，则我们能够看到随着模型的增大模型的表现有多好。\n",
    "\n",
    "### 模型复杂度的实际使用\n",
    "\n",
    "可利用一个可视化工具来帮助找出优化模型的方法。利用gridsearch 对模型进行微调以获得更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测波士顿房价\n",
    "\n",
    "通过完成此项目您将会学习（并最终知道）以下知识：\n",
    "\n",
    "- 如何使用 NumPy 调查数据集的潜在特征。\n",
    "- 如何分析各种学习性能图以了解方差和偏差。\n",
    "- 如何确定用于从未看见的数据进行预测的最佳猜测模型。\n",
    "- 如何评估模型使用之前的数据处理未看见数据的表现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
